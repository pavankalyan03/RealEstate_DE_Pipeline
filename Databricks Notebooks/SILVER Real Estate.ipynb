{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b2727f-cd10-4616-a33f-8f291a2e6487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_properties = spark.read.table(\"df_properties\")\n",
    "df_config = spark.read.table(\"df_config\")\n",
    "df_tags = spark.read.table(\"df_tags\")\n",
    "df_amenities = spark.read.table(\"df_amenities\")\n",
    "df_places = spark.read.table(\"df_places\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d286b2a5-464d-49a8-a59b-27989ed23669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad356e52-2f44-4f5d-baf8-8a4601c0bbdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_properties.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601a7d4c-9d49-4678-9eb8-b857e6d927e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Clean Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4008eebb-bd80-45b6-a626-d0a7e24b88cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_properties_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c6e2043-d534-4054-891f-a905c452a806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_config.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "240a74ea-b0d5-4ab6-b4c5-a50a03fad0f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- address: string (nullable = true)\n |-- bathrooms: integer (nullable = false)\n |-- bedrooms: integer (nullable = false)\n |-- brochure_url: string (nullable = false)\n |-- city: string (nullable = true)\n |-- currency: string (nullable = true)\n |-- emi: string (nullable = true)\n |-- from_url: string (nullable = true)\n |-- id: string (nullable = true)\n |-- image_url: string (nullable = false)\n |-- is_active_property: boolean (nullable = true)\n |-- is_most_contacted: boolean (nullable = true)\n |-- latitude: double (nullable = true)\n |-- locality: string (nullable = true)\n |-- long_address: string (nullable = true)\n |-- longitude: double (nullable = true)\n |-- max_price: long (nullable = true)\n |-- min_price: long (nullable = true)\n |-- name: string (nullable = true)\n |-- region: string (nullable = true)\n |-- seller_is_paid: boolean (nullable = true)\n |-- seller_name: string (nullable = true)\n |-- seller_phone: string (nullable = true)\n |-- seller_type: string (nullable = true)\n |-- state: string (nullable = true)\n |-- subtitle: string (nullable = true)\n |-- url: string (nullable = true)\n |-- description_clean: string (nullable = false)\n |-- avg_price_per_sqft: double (nullable = false)\n |-- min_price_val: double (nullable = true)\n |-- max_price_val: double (nullable = true)\n |-- property_price: double (nullable = false)\n |-- property_area_sqft: double (nullable = true)\n |-- min_size: double (nullable = false)\n |-- max_size: double (nullable = false)\n |-- posted_date: timestamp (nullable = true)\n |-- possession_date: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType, DateType, TimestampType, BooleanType, StructType, StructField\n",
    "import re\n",
    "\n",
    "# 3.1 Helper functions for parsing\n",
    "def parse_amount_indian(s: str):\n",
    "    \"\"\"\n",
    "    Parse a string like '₹7.2 K', '76.75 L', '1.44 Cr', or plain numbers with commas.\n",
    "    Returns a float (absolute rupees), or None if unparsable.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    # Remove currency symbol and any whitespace\n",
    "    s_clean = s.replace('₹', '').replace(',', '').strip()\n",
    "    # Regex to capture number and optional suffix\n",
    "    m = re.match(r'([\\d\\.]+)\\s*([KkLl]|Lakh|lakh|Cr|cr|Crore|crore)?', s_clean)\n",
    "    if not m:\n",
    "        # Try parse plain float\n",
    "        try:\n",
    "            return float(s_clean)\n",
    "        except:\n",
    "            return None\n",
    "    num = float(m.group(1))\n",
    "    suffix = m.group(2)\n",
    "    if not suffix:\n",
    "        return num\n",
    "    suffix = suffix.lower()\n",
    "    if suffix in ('k',):\n",
    "        return num * 1e3\n",
    "    if suffix in ('l', 'lakh'):\n",
    "        return num * 1e5\n",
    "    if suffix in ('cr', 'crore'):\n",
    "        return num * 1e7\n",
    "    # default\n",
    "    return num\n",
    "\n",
    "def parse_price_range(s: str):\n",
    "    \"\"\"\n",
    "    Parse a range string like '₹76.75 L - 1.44 Cr' or '76.75 L - 1.44 Cr'.\n",
    "    Returns tuple (min_val, max_val) in rupees, or (None, None) if unparsable.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return (None, None)\n",
    "    # Remove currency symbols globally; but keep suffixes\n",
    "    s_clean = s.replace('₹', '').strip()\n",
    "    # Split on hyphen\n",
    "    parts = [p.strip() for p in re.split(r'-', s_clean)]\n",
    "    if len(parts) != 2:\n",
    "        return (None, None)\n",
    "    low, high = parts\n",
    "    low_val = parse_amount_indian(low)\n",
    "    high_val = parse_amount_indian(high)\n",
    "    return (low_val, high_val)\n",
    "\n",
    "# Register UDFs with corrected StructType import\n",
    "parse_amount_udf = F.udf(lambda x: parse_amount_indian(x) if x else None, DoubleType())\n",
    "parse_price_range_udf = F.udf(lambda x: parse_price_range(x) if x else (None, None), \n",
    "                              StructType([\n",
    "                                  StructField(\"min_val\", DoubleType()),\n",
    "                                  StructField(\"max_val\", DoubleType())\n",
    "                              ]))\n",
    "\n",
    "# Rest of your code remains the same...\n",
    "\n",
    "# 3.2 Start cleaning pipeline\n",
    "df = df_properties  # your input DataFrame\n",
    "\n",
    "# 3.2.1 Remove duplicate IDs\n",
    "df = df.dropDuplicates([\"id\"])\n",
    "\n",
    "# 3.2.2 Trim string columns globally (optional helper)\n",
    "string_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() == \"string\"]\n",
    "for col in string_cols:\n",
    "    df = df.withColumn(col, F.when(F.col(col).isNotNull(), F.trim(F.col(col))).otherwise(F.col(col)))\n",
    "\n",
    "# 3.2.3 Clean description: remove HTML tags\n",
    "df = df.withColumn(\n",
    "    \"description_clean\",\n",
    "    F.when(\n",
    "        F.col(\"description\").isNotNull(),\n",
    "        F.trim(F.regexp_replace(F.regexp_replace(F.col(\"description\"), \"<.*?>\", \"\"), \"\\\\s+\", \" \"))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# 3.2.4 Parse numeric fields\n",
    "\n",
    "# avg_price_per_sqft: remove \"/sq.ft\" suffix, parse amount\n",
    "df = df.withColumn(\n",
    "    \"avg_price_per_sqft_val\",\n",
    "    parse_amount_udf(\n",
    "        F.when(\n",
    "            F.col(\"avg_price_per_sqft\").isNotNull(),\n",
    "            F.trim(F.regexp_replace(F.col(\"avg_price_per_sqft\"), r\"/.*$\", \"\"))\n",
    "        ).otherwise(None)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# price_display_value into min_price_val, max_price_val\n",
    "df = df.withColumn(\"price_range_struct\", parse_price_range_udf(F.col(\"price_display_value\"))) \\\n",
    "       .withColumn(\"min_price_val\", F.col(\"price_range_struct.min_val\")) \\\n",
    "       .withColumn(\"max_price_val\", F.col(\"price_range_struct.max_val\")) \\\n",
    "       .drop(\"price_range_struct\")\n",
    "\n",
    "# property_price: parse string values like '76.75 L'\n",
    "df = df.withColumn(\"property_price_val\", parse_amount_udf(F.col(\"property_price\")))\n",
    "\n",
    "# property_area: e.g., '1066 sq.ft' → numeric\n",
    "df = df.withColumn(\"property_area_val\",\n",
    "                   F.when(F.col(\"property_area\").isNotNull(),\n",
    "                          F.regexp_extract(F.col(\"property_area\"), r\"([\\d\\.]+)\", 1).cast(DoubleType())\n",
    "                         ).otherwise(None)\n",
    "                  )\n",
    "\n",
    "# size_range: '1066 - 1999 sq.ft.' → min_size, max_size\n",
    "df = df.withColumn(\"size_range_clean\", F.regexp_replace(F.col(\"size_range\"), \"sq\\\\.ft\\\\.?\", \"\")) \\\n",
    "       .withColumn(\"size_parts\", F.split(F.col(\"size_range_clean\"), r\"\\s*-\\s*\")) \\\n",
    "       .withColumn(\"min_size\", \n",
    "                   F.when(F.size(F.col(\"size_parts\")) >= 1, \n",
    "                          F.col(\"size_parts\").getItem(0).cast(DoubleType())).otherwise(None)) \\\n",
    "       .withColumn(\"max_size\", \n",
    "                   F.when(F.size(F.col(\"size_parts\")) >= 2, \n",
    "                          F.col(\"size_parts\").getItem(1).cast(DoubleType())).otherwise(None)) \\\n",
    "       .drop(\"size_range_clean\", \"size_parts\")\n",
    "\n",
    "# latitude, longitude: cast to Double\n",
    "df = df.withColumn(\"latitude\", F.col(\"latitude\").cast(DoubleType())) \\\n",
    "       .withColumn(\"longitude\", F.col(\"longitude\").cast(DoubleType()))\n",
    "\n",
    "# bathrooms, bedrooms: cast to Integer\n",
    "df = df.withColumn(\"bedrooms\", F.col(\"bedrooms\").cast(IntegerType())) \n",
    "    #    .withColumn(\"bathrooms\", F.col(\"bathrooms\").cast(IntegerType()))\n",
    "\n",
    "# 3.2.5 Parse dates\n",
    "\n",
    "# posted_date: ISO format \"2019-08-13T00:00:33Z\"\n",
    "df = df.withColumn(\n",
    "    \"posted_timestamp\", \n",
    "    F.to_timestamp(F.col(\"posted_date\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
    ")\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"possession_date_parsed\",\n",
    "    F.when(\n",
    "        F.col(\"possession_date\").isNotNull(),\n",
    "        F.to_date(\n",
    "            F.concat(F.lit(\"01 \"), F.col(\"possession_date\")),\n",
    "            \"dd MMM, yyyy\"\n",
    "        )\n",
    "    ).otherwise(F.to_date(F.col(\"posted_timestamp\")))\n",
    ")\n",
    "\n",
    "\n",
    "# 3.2.6 Cast boolean-like columns\n",
    "for bool_col in [\"is_active_property\", \"is_most_contacted\", \"seller_is_paid\"]:\n",
    "    df = df.withColumn(bool_col,\n",
    "                       F.when(F.col(bool_col).isin(True, False), F.col(bool_col).cast(BooleanType()))\n",
    "                        .when(F.lower(F.col(bool_col)) == \"true\", F.lit(True))\n",
    "                        .when(F.lower(F.col(bool_col)) == \"false\", F.lit(False))\n",
    "                        .otherwise(None)\n",
    "                      )\n",
    "\n",
    "# 3.2.7 Trim/clean categorical columns\n",
    "for cat_col in [\"region\", \"city\", \"state\", \"seller_type\", \"seller_name\", \"locality\"]:\n",
    "    df = df.withColumn(cat_col, \n",
    "                       F.when(F.col(cat_col).isNotNull(), F.initcap(F.col(cat_col))).otherwise(F.col(cat_col))\n",
    "                      )\n",
    "\n",
    "# 3.2.8 Logical consistency: drop or flag rows where min_price > max_price\n",
    "df = df.withColumn(\n",
    "    \"price_consistent\", \n",
    "    F.when(\n",
    "        (F.col(\"min_price_val\").isNotNull() & \n",
    "         F.col(\"max_price_val\").isNotNull() &\n",
    "         (F.col(\"min_price_val\") <= F.col(\"max_price_val\"))),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# Option: drop inconsistent, or keep but flag\n",
    "df = df.filter(F.col(\"price_consistent\") == True).drop(\"price_consistent\")\n",
    "\n",
    "# 3.2.9 Handle missing critical values\n",
    "df = df.filter(F.col(\"id\").isNotNull() & \n",
    "              F.col(\"latitude\").isNotNull() & \n",
    "              F.col(\"longitude\").isNotNull())\n",
    "\n",
    "\n",
    "# For bedrooms/bathrooms null: fill with -1\n",
    "df = df.withColumn(\"bathrooms\", F.coalesce(F.col(\"bathrooms\"), F.lit(-1))) \\\n",
    "       .withColumn(\"bedrooms\", F.coalesce(F.col(\"bedrooms\"), F.lit(-1)))\n",
    "\n",
    "# 3.2.10 Drop intermediate helper columns if desired\n",
    "df_clean = df.drop(\"avg_price_per_sqft\", \"price_display_value\", \n",
    "                   \"property_price\", \"property_area\", \"description\",\n",
    "                   \"size_range\", \"posted_date\", \"possession_date\")\n",
    "\n",
    "# Rename parsed columns\n",
    "df_clean = df_clean.withColumnRenamed(\"avg_price_per_sqft_val\", \"avg_price_per_sqft\") \\\n",
    "                   .withColumnRenamed(\"property_area_val\", \"property_area_sqft\") \\\n",
    "                   .withColumnRenamed(\"property_price_val\", \"property_price\") \\\n",
    "                   .withColumnRenamed(\"posted_timestamp\", \"posted_date\") \\\n",
    "                   .withColumnRenamed(\"possession_date_parsed\", \"possession_date\")\n",
    "\n",
    "df_clean = df_clean.withColumn(\"avg_price_per_sqft\", F.col(\"avg_price_per_sqft\").cast(DoubleType()))\n",
    "df_clean = df_clean.withColumn(\"property_price\", F.col(\"property_price\").cast(DoubleType()))\n",
    "df_clean = df_clean.withColumn(\"min_size\", F.col(\"min_size\").cast(DoubleType()))\n",
    "df_clean = df_clean.withColumn(\"max_size\", F.col(\"max_size\").cast(DoubleType()))\n",
    "median_avg_price = df_clean.approxQuantile(\"avg_price_per_sqft\", [0.5], 0.01)[0]\n",
    "median_property_price = df_clean.approxQuantile(\"property_price\", [0.5], 0.01)[0]\n",
    "median_min_size = df_clean.approxQuantile(\"min_size\", [0.5], 0.01)[0]\n",
    "median_max_size = df_clean.approxQuantile(\"max_size\", [0.5], 0.01)[0]\n",
    "\n",
    "# Treating null values after thorough observing inside data.\n",
    "df_clean = df_clean.fillna({\n",
    "    \"avg_price_per_sqft\": median_avg_price,\n",
    "    \"property_price\": median_property_price,\n",
    "    \"min_size\": median_min_size,\n",
    "    \"max_size\": median_max_size,\n",
    "    \"brochure_url\": \"\",\n",
    "    \"image_url\": \"\",\n",
    "    \"description_clean\": \"\"\n",
    "})\n",
    "\n",
    "# 3.2.11 Final type enforcement\n",
    "target_schema = {\n",
    "    \"id\": \"string\",\n",
    "    \"address\": \"string\",\n",
    "    \"long_address\": \"string\",\n",
    "    \"locality\": \"string\",\n",
    "    \"city\": \"string\",\n",
    "    \"region\": \"string\",\n",
    "    \"state\": \"string\",\n",
    "    \"latitude\": \"double\",\n",
    "    \"longitude\": \"double\",\n",
    "    \"bedrooms\": \"int\",\n",
    "    \"bathrooms\": \"int\",\n",
    "    \"avg_price_per_sqft\": \"double\",\n",
    "    \"min_price_val\": \"double\",\n",
    "    \"max_price_val\": \"double\",\n",
    "    \"property_price\": \"double\",\n",
    "    \"property_area_sqft\": \"double\",\n",
    "    \"min_size\": \"double\",\n",
    "    \"max_size\": \"double\",\n",
    "    \"posted_date\": \"timestamp\",\n",
    "    \"possession_date\": \"date\",\n",
    "    \"description_clean\": \"string\",\n",
    "    \"brochure_url\": \"string\",\n",
    "    \"from_url\": \"string\",\n",
    "    \"url\": \"string\",\n",
    "    \"is_active_property\": \"boolean\",\n",
    "    \"is_most_contacted\": \"boolean\",\n",
    "    \"seller_is_paid\": \"boolean\",\n",
    "    \"seller_name\": \"string\",\n",
    "    \"seller_phone\": \"string\",\n",
    "    \"seller_type\": \"string\",\n",
    "    \"subtitle\": \"string\"\n",
    "}\n",
    "\n",
    "for col_name, dtype in target_schema.items():\n",
    "    if col_name in df_clean.columns:\n",
    "        df_clean = df_clean.withColumn(col_name, F.col(col_name).cast(dtype))\n",
    "\n",
    "\n",
    "# 3.2.12 Show cleaned schema & sample\n",
    "df_clean.printSchema()\n",
    "# df_clean.show(5, truncate=False)\n",
    "\n",
    "df_properties_cleaned = df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee67825-ab51-4c79-afdf-7eeba7fd3dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------------+---------------+---------------+\n|id    |config_type|config_category|min_price_lakhs|max_price_lakhs|\n+------+-----------+---------------+---------------+---------------+\n|269627|3 BHK      |Apartment      |61.88          |83.66          |\n|284804|3 BHK      |Apartment      |123.0          |134.0          |\n|182229|3 BHK      |Apartment      |225.0          |296.0          |\n|325921|3 BHK      |Apartment      |150.0          |244.0          |\n|227035|3 BHK      |Apartment      |158.0          |193.0          |\n|125374|3 BHK      |Apartment      |154.0          |194.0          |\n|228309|3 BHK      |Apartment      |238.0          |316.0          |\n|284131|3 BHK      |Apartment      |90.0           |90.0           |\n|251066|3 BHK      |Apartment      |89.6           |92.4           |\n|290575|3 BHK      |Apartment      |77.12          |77.12          |\n+------+-----------+---------------+---------------+---------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    split, col, array_position, size, concat_ws, when,\n",
    "    initcap, lit, slice as spark_slice, trim, coalesce\n",
    ")\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to convert 'L' and 'Cr' values to float Lakhs (as before)\n",
    "def parse_price_to_lakhs(value):\n",
    "    try:\n",
    "        value = value.strip()\n",
    "        if \"Cr\" in value:\n",
    "            return float(value.replace(\"Cr\", \"\").strip()) * 100\n",
    "        elif \"L\" in value:\n",
    "            return float(value.replace(\"L\", \"\").strip())\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "parse_price_udf = udf(parse_price_to_lakhs, FloatType())\n",
    "\n",
    "df = df_config\n",
    "\n",
    "# Drop duplicates and price parsing as before\n",
    "df = df.dropDuplicates([\"id\", \"config_type\", \"price_range\"])\n",
    "df = df.withColumn(\"price_parts\", split(col(\"price_range\"), \"-\"))\n",
    "df = df.withColumn(\"min_price_lakhs\", parse_price_udf(trim(col(\"price_parts\")[0])))\n",
    "df = df.withColumn(\n",
    "    \"max_price_lakhs\",\n",
    "    when(col(\"price_parts\").getItem(1).isNotNull(),\n",
    "         parse_price_udf(trim(col(\"price_parts\")[1])))\n",
    "    .otherwise(col(\"min_price_lakhs\"))\n",
    ")\n",
    "\n",
    "# --- NEW: parse config_type into size-format and category ---\n",
    "\n",
    "# 1. split on underscore\n",
    "df = df.withColumn(\"config_parts\", split(col(\"config_type\"), \"_\"))\n",
    "\n",
    "# 2. find position of \"bhk\" in array (1-based index). If absent, returns 0.\n",
    "df = df.withColumn(\"pos_bhk\", array_position(col(\"config_parts\"), \"bhk\"))\n",
    "\n",
    "# 3. derive parsed config_type (size/format)\n",
    "#    - if pos_bhk > 1: take element at pos_bhk-1 as size, append \"BHK\"\n",
    "#    - elif first part is \"studio\": use \"Studio\"\n",
    "#    - else: no size-format → null\n",
    "df = df.withColumn(\n",
    "    \"config_type_parsed\",\n",
    "    when(col(\"pos_bhk\") > 1,\n",
    "         concat_ws(\" \", col(\"config_parts\")[col(\"pos_bhk\") - 2], lit(\"BHK\"))\n",
    "    ).when(col(\"config_parts\")[0] == \"studio\",\n",
    "         initcap(col(\"config_parts\")[0])\n",
    "    ).otherwise(lit(None))\n",
    ")\n",
    "\n",
    "# 4. derive parsed config_category\n",
    "#    - if pos_bhk > 0 and there are parts after \"bhk\": slice those for category\n",
    "#    - elif pos_bhk > 0 but bhk is last part: no category (null)\n",
    "#    - elif first part is \"studio\" and there are parts after: those form category\n",
    "#    - else: treat entire joined parts as category\n",
    "# Note: spark_slice is 1-based: spark_slice(array, startPos, length)\n",
    "# For slicing after bhk: startPos = pos_bhk + 1, length = size(config_parts) - pos_bhk\n",
    "df = df.withColumn(\n",
    "    \"config_category_parsed\",\n",
    "    when(\n",
    "        (col(\"pos_bhk\") > 0) & (size(col(\"config_parts\")) > col(\"pos_bhk\")),\n",
    "        initcap(\n",
    "            concat_ws(\" \",\n",
    "                      spark_slice(\n",
    "                          col(\"config_parts\"),\n",
    "                          col(\"pos_bhk\") + 1,\n",
    "                          size(col(\"config_parts\")) - col(\"pos_bhk\")\n",
    "                      )\n",
    "            )\n",
    "        )\n",
    "    ).when(\n",
    "        (col(\"pos_bhk\") > 0) & (size(col(\"config_parts\")) == col(\"pos_bhk\")),\n",
    "        lit(None)\n",
    "    ).when(\n",
    "        (col(\"config_parts\")[0] == \"studio\") & (size(col(\"config_parts\")) > 1),\n",
    "        initcap(\n",
    "            concat_ws(\" \",\n",
    "                      spark_slice(col(\"config_parts\"), 2, size(col(\"config_parts\")) - 1)\n",
    "            )\n",
    "        )\n",
    "    ).otherwise(\n",
    "        # join all parts e.g. [\"plot\"] or [\"builder\",\"floor\"] or any other single/multi-part\n",
    "        initcap(concat_ws(\" \", col(\"config_parts\")))\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. replace original columns: overwrite config_type and add config_category\n",
    "df = df.withColumnRenamed(\"config_type\", \"config_type_original\") \\\n",
    "       .withColumnRenamed(\"config_type_parsed\", \"config_type\") \\\n",
    "       .withColumnRenamed(\"config_category_parsed\", \"config_category\")\n",
    "\n",
    "# 6. drop helpers\n",
    "df = df.drop(\"config_parts\", \"pos_bhk\")\n",
    "\n",
    "df = df.withColumn(\"config_type\", coalesce(col(\"config_type\"), lit(\"Other\")))\n",
    "df = df.withColumn(\"config_category\", coalesce(col(\"config_category\"), lit(\"Unknown\")))\n",
    "\n",
    "# Final cleaned DataFrame\n",
    "df_config_cleaned = df.select(\"id\", \"config_type\", \"config_category\", \"min_price_lakhs\", \"max_price_lakhs\")\n",
    "\n",
    "df_config_cleaned.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f0b4df-279b-4ee7-a86c-f378be582188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9704"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_config.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42356b5c-d12a-43d0-9bcf-fcc028cec2f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT * FROM df_config WHERE ID = 269574;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b2f29a-563d-4b88-afa9-e954225e051b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_tags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f92fc2-b965-4d58-9797-dd5fc7ff9b08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+--------------+---------------------+----------------+---------------+\n|    id|is_rera_approved|is_new_booking|is_under_construction|is_ready_to_move|has_project_tag|\n+------+----------------+--------------+---------------------+----------------+---------------+\n|265095|            true|          true|                false|            true|          false|\n|250487|            true|         false|                false|            true|           true|\n+------+----------------+--------------+---------------------+----------------+---------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, lit, trim, regexp_extract, max as spark_max\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Standardize tag values (trim spaces)\n",
    "df_tags = df_tags.withColumn(\"tag\", trim(col(\"tag\")))\n",
    "\n",
    "# Step 2: Define features using conditional aggregation\n",
    "df_tags_cleaned = df_tags.groupBy(\"id\").agg(\n",
    "    F.max(when(col(\"tag\") == \"RERA Approved\", lit(True)).otherwise(lit(False))).alias(\"is_rera_approved\"),\n",
    "    F.max(when(col(\"tag\") == \"New Booking\", lit(True)).otherwise(lit(False))).alias(\"is_new_booking\"),\n",
    "    F.max(when(col(\"tag\") == \"Under Construction\", lit(True)).otherwise(lit(False))).alias(\"is_under_construction\"),\n",
    "    F.max(when(col(\"tag\") == \"Ready to Move\", lit(True)).otherwise(lit(False))).alias(\"is_ready_to_move\"),\n",
    "    F.max(when(col(\"tag\") == \"Project\", lit(True)).otherwise(lit(False))).alias(\"has_project_tag\"),\n",
    "    \n",
    "    # Extract possession date from tag like: \"Possession: Mar 2026\"\n",
    "    # F.max(\n",
    "    #     when(col(\"tag\").rlike(\"Possession:.*\"),\n",
    "    #          F.to_date(regexp_extract(col(\"tag\"), \"Possession: (\\\\w+ \\\\d{4})\", 1), \"MMM yyyy\"))\n",
    "    # ).alias(\"possession_date\")\n",
    ")\n",
    "\n",
    "# df_tags_cleaned = df_tags_cleaned.fillna({\"possession_date\" : \" \"})\n",
    "\n",
    "df_tags_cleaned.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c60d2ce-0d7b-4455-ac3d-ede3a1e22449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "43579"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tags.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d76b84c7-ccf7-46f3-91fc-3d2cca3f9b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_amenities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74343417-1a89-47b0-8b70-cadf081a5a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------------------+-------+--------+-----------+--------+\n|    id|Gas Pipeline|has_gated_community|has_gym|has_lift|has_parking|has_pool|\n+------+------------+-------------------+-------+--------+-----------+--------+\n|201828|       false|              false|  false|   false|       true|   false|\n|201103|       false|              false|  false|   false|       true|   false|\n+------+------------+-------------------+-------+--------+-----------+--------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, trim, lower, when, max as spark_max\n",
    "\n",
    "# Step 1: Clean 'amenity' column (strip whitespace)\n",
    "df_amenities = df_amenities.withColumn(\"amenity\", trim(col(\"amenity\")))\n",
    "\n",
    "# Step 2: Normalize amenity names if needed (optional)\n",
    "# Example: standardize casing (e.g., lower case then map back if needed)\n",
    "# Step 3: Create binary flags\n",
    "df_amenity_clean = df_amenities.withColumn(\"value\", lit(True))\n",
    "\n",
    "# Step 4: Pivot the amenity values\n",
    "df_pivoted = df_amenity_clean.groupBy(\"id\").pivot(\"amenity\").agg(spark_max(\"value\"))\n",
    "\n",
    "# Step 5: Rename columns to meaningful boolean feature names\n",
    "df_pivoted = df_pivoted \\\n",
    "    .withColumnRenamed(\"Pool\", \"has_pool\") \\\n",
    "    .withColumnRenamed(\"Gym\", \"has_gym\") \\\n",
    "    .withColumnRenamed(\"Lift\", \"has_lift\") \\\n",
    "    .withColumnRenamed(\"Parking\", \"has_parking\") \\\n",
    "    .withColumnRenamed(\"Gated Community\", \"has_gated_community\")\n",
    "\n",
    "# Fill nulls with False\n",
    "df_amenities_cleaned = df_pivoted.fillna(False)\n",
    "\n",
    "df_amenities_cleaned.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9135f56d-6ede-4dbd-a8ee-3559808d87d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19741"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amenities.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "626a275d-037a-4a8d-84bf-697dea157dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_places.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10308d89-d4d3-496d-b466-31c0115e826d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+-----------+---------------+\n|    id|   place_type|          place_name|distance_km|travel_time_min|\n+------+-------------+--------------------+-----------+---------------+\n|103088|       school|The Hyderabad Pub...|        1.8|             -1|\n|285010|   restaurant|  Peacock Restaurant|       2.24|             -1|\n|343119|     hospital|    Surekha Hospital|       0.39|              1|\n|287751|shopping_mall|Shopping Complex ...|       3.38|             90|\n|281814|       school|Sadhu Vaswani Int...|       0.66|             -1|\n+------+-------------+--------------------+-----------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim, lower, round, when\n",
    "\n",
    "\n",
    "df_places_cleaned = df_places\n",
    "\n",
    "# Step 1: Trim and lower case place_type and place_name\n",
    "df_places_cleaned = df_places_cleaned \\\n",
    "    .withColumn(\"place_type\", trim(lower(col(\"place_type\")))) \\\n",
    "    .withColumn(\"place_name\", trim(col(\"place_name\")))\n",
    "\n",
    "# Step 2: Cast numeric columns\n",
    "df_places_cleaned = df_places_cleaned \\\n",
    "    .withColumn(\"distance_km\", round(col(\"distance_km\").cast(\"double\"), 2)) \\\n",
    "    .withColumn(\"travel_time_min\", col(\"travel_time_min\").cast(\"int\"))\n",
    "\n",
    "# Step 3: Handle missing travel_time_min\n",
    "df_places_cleaned = df_places_cleaned.withColumn(\n",
    "    \"travel_time_min\",\n",
    "    when(col(\"travel_time_min\").isNull(), -1).otherwise(col(\"travel_time_min\"))\n",
    ")\n",
    "\n",
    "# Step 4: Optional deduplication\n",
    "df_places_cleaned = df_places_cleaned.dropDuplicates([\"id\", \"place_type\", \"place_name\", \"distance_km\"])\n",
    "\n",
    "df_places_cleaned.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac94f33-f09c-4bd2-81f8-cf76b5a2550a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_properties_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435c1ac5-f6e3-43f6-81d5-3e32dd677f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_places_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb33c9bf-4610-4999-9224-5bed0a702b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sfOptions = {\n",
    "    \"sfURL\": \"YQRWNWT-DR42610.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"MASTER\",\n",
    "    \"sfPassword\": \"6305524392Pavan@\",\n",
    "    \"sfDatabase\": \"REALESTATE\",\n",
    "    \"sfSchema\": \"SILVER\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\"  # Optional\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0981ccf6-3a02-445b-b967-f2d83ffe49f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_properties_cleaned.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"properties\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "df_amenities_cleaned.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"property_amenities\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "\n",
    "df_config_cleaned.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"property_configurations\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "\n",
    "df_tags_cleaned.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"property_status\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "\n",
    "df_places_cleaned.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"property_nearby_places\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca860e9d-f1d0-4931-9c71-552bfccbae91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.types import DoubleType, IntegerType, DateType, TimestampType, BooleanType, StructType, StructField\n",
    "# import re\n",
    "\n",
    "# # -------------------- Helper Functions --------------------\n",
    "\n",
    "# def parse_amount_indian(s: str):\n",
    "#     if not s:\n",
    "#         return None\n",
    "#     s_clean = s.replace('₹', '').replace(',', '').strip()\n",
    "#     m = re.match(r'([\\d\\.]+)\\s*([KkLl]|Lakh|lakh|Cr|cr|Crore|crore)?', s_clean)\n",
    "#     if not m:\n",
    "#         try:\n",
    "#             return float(s_clean)\n",
    "#         except:\n",
    "#             return None\n",
    "#     num = float(m.group(1))\n",
    "#     suffix = m.group(2)\n",
    "#     if not suffix:\n",
    "#         return num\n",
    "#     suffix = suffix.lower()\n",
    "#     if suffix in ('k',):\n",
    "#         return num * 1e3\n",
    "#     if suffix in ('l', 'lakh'):\n",
    "#         return num * 1e5\n",
    "#     if suffix in ('cr', 'crore'):\n",
    "#         return num * 1e7\n",
    "#     return num\n",
    "\n",
    "# def parse_price_range(s: str):\n",
    "#     if not s:\n",
    "#         return (None, None)\n",
    "#     s_clean = s.replace('₹', '').strip()\n",
    "#     parts = [p.strip() for p in re.split(r'-', s_clean)]\n",
    "#     if len(parts) != 2:\n",
    "#         return (None, None)\n",
    "#     low_val = parse_amount_indian(parts[0])\n",
    "#     high_val = parse_amount_indian(parts[1])\n",
    "#     return (low_val, high_val)\n",
    "\n",
    "# # Register UDFs\n",
    "# parse_amount_udf = F.udf(lambda x: parse_amount_indian(x) if x else None, DoubleType())\n",
    "# parse_price_range_udf = F.udf(\n",
    "#     lambda x: parse_price_range(x) if x else (None, None),\n",
    "#     StructType([\n",
    "#         StructField(\"min_val\", DoubleType()),\n",
    "#         StructField(\"max_val\", DoubleType())\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "# # -------------------- Data Cleaning Pipeline --------------------\n",
    "\n",
    "# df = df_properties\n",
    "# print(f\"Initial count: {df.count()}\")\n",
    "\n",
    "# # 1. Drop duplicate IDs\n",
    "# df = df.dropDuplicates([\"id\"])\n",
    "# print(f\"After dropDuplicates: {df.count()}\")\n",
    "\n",
    "# # 2. Trim all string columns\n",
    "# string_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() == \"string\"]\n",
    "# for col in string_cols:\n",
    "#     df = df.withColumn(col, F.when(F.col(col).isNotNull(), F.trim(F.col(col))).otherwise(F.col(col)))\n",
    "\n",
    "# # 3. Clean HTML from description\n",
    "# df = df.withColumn(\n",
    "#     \"description_clean\",\n",
    "#     F.when(\n",
    "#         F.col(\"description\").isNotNull(),\n",
    "#         F.trim(F.regexp_replace(F.regexp_replace(F.col(\"description\"), \"<.*?>\", \"\"), \"\\\\s+\", \" \"))\n",
    "#     ).otherwise(None)\n",
    "# )\n",
    "\n",
    "# # 4. Parse avg_price_per_sqft\n",
    "# df = df.withColumn(\n",
    "#     \"avg_price_per_sqft_val\",\n",
    "#     parse_amount_udf(\n",
    "#         F.when(\n",
    "#             F.col(\"avg_price_per_sqft\").isNotNull(),\n",
    "#             F.trim(F.regexp_replace(F.col(\"avg_price_per_sqft\"), r\"/.*$\", \"\"))\n",
    "#         ).otherwise(None)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # 5. Parse price_display_value into min/max\n",
    "# df = df.withColumn(\"price_range_struct\", parse_price_range_udf(F.col(\"price_display_value\"))) \\\n",
    "#        .withColumn(\"min_price_val\", F.col(\"price_range_struct.min_val\")) \\\n",
    "#        .withColumn(\"max_price_val\", F.col(\"price_range_struct.max_val\")) \\\n",
    "#        .drop(\"price_range_struct\")\n",
    "# print(f\"After parsing price_display_value: {df.count()}\")\n",
    "\n",
    "# # --- After parsing price_display_value and filtering price consistency ---\n",
    "\n",
    "# # Log how many nulls before imputation\n",
    "# null_min = df.filter(F.col(\"min_price_val\").isNull()).count()\n",
    "# null_max = df.filter(F.col(\"max_price_val\").isNull()).count()\n",
    "# print(f\"Before price imputation: null min_price_val = {null_min}, null max_price_val = {null_max}, total rows = {df.count()}\")\n",
    "\n",
    "# # 1. Compute global median ratio (max/min) among rows where both exist\n",
    "# ratio_df = df.filter(F.col(\"min_price_val\").isNotNull() & F.col(\"max_price_val\").isNotNull()) \\\n",
    "#              .withColumn(\"price_ratio\", F.col(\"max_price_val\") / F.col(\"min_price_val\"))\n",
    "# # If there are enough rows:\n",
    "# if ratio_df.count() > 0:\n",
    "#     ratio_median_row = ratio_df.selectExpr(\"percentile_approx(price_ratio, 0.5) as med_ratio\").collect()[0]\n",
    "#     ratio_median = ratio_median_row[\"med_ratio\"]\n",
    "# else:\n",
    "#     ratio_median = None\n",
    "# print(f\"Global median price ratio (max/min): {ratio_median}\")\n",
    "\n",
    "# # 2. Compute group-level medians by locality (or another grouping column you prefer)\n",
    "# #    If locality has many distinct values, ensure performance is acceptable.\n",
    "# group_medians = df.groupBy(\"locality\").agg(\n",
    "#     F.expr(\"percentile_approx(min_price_val, 0.5)\").alias(\"med_min_loc\"),\n",
    "#     F.expr(\"percentile_approx(max_price_val, 0.5)\").alias(\"med_max_loc\")\n",
    "# )\n",
    "# df = df.join(group_medians, on=\"locality\", how=\"left\")\n",
    "\n",
    "# # 3. Impute min_price_val:\n",
    "# #    - If min is null but max exists and ratio_median is known: set min = max / ratio_median\n",
    "# #    - Else if min is null: use med_min_loc\n",
    "# df = df.withColumn(\n",
    "#     \"min_price_val_imputed\",\n",
    "#     F.when(\n",
    "#         F.col(\"min_price_val\").isNull() & F.col(\"max_price_val\").isNotNull() & F.lit(ratio_median).isNotNull(),\n",
    "#         F.col(\"max_price_val\") / F.lit(ratio_median)\n",
    "#     ).when(\n",
    "#         F.col(\"min_price_val\").isNull(),\n",
    "#         F.col(\"med_min_loc\")\n",
    "#     ).otherwise(F.col(\"min_price_val\"))\n",
    "# )\n",
    "\n",
    "# # 4. Impute max_price_val:\n",
    "# #    - If max is null but min exists and ratio_median is known: set max = min * ratio_median\n",
    "# #    - Else if max is null: use med_max_loc\n",
    "# df = df.withColumn(\n",
    "#     \"max_price_val_imputed\",\n",
    "#     F.when(\n",
    "#         F.col(\"max_price_val\").isNull() & F.col(\"min_price_val\").isNotNull() & F.lit(ratio_median).isNotNull(),\n",
    "#         F.col(\"min_price_val\") * F.lit(ratio_median)\n",
    "#     ).when(\n",
    "#         F.col(\"max_price_val\").isNull(),\n",
    "#         F.col(\"med_max_loc\")\n",
    "#     ).otherwise(F.col(\"max_price_val\"))\n",
    "# )\n",
    "\n",
    "# # 5. For any remaining nulls (e.g., group median was null or ratio_median was None), fallback to global medians:\n",
    "# global_medians = df.selectExpr(\n",
    "#     \"percentile_approx(min_price_val, 0.5) as global_med_min\",\n",
    "#     \"percentile_approx(max_price_val, 0.5) as global_med_max\"\n",
    "# ).collect()[0]\n",
    "# global_med_min = global_medians[\"global_med_min\"]\n",
    "# global_med_max = global_medians[\"global_med_max\"]\n",
    "# print(f\"Global medians fallback: min = {global_med_min}, max = {global_med_max}\")\n",
    "\n",
    "# df = df.withColumn(\n",
    "#     \"min_price_val_imputed\",\n",
    "#     F.when(F.col(\"min_price_val_imputed\").isNull(), F.lit(global_med_min)).otherwise(F.col(\"min_price_val_imputed\"))\n",
    "# ).withColumn(\n",
    "#     \"max_price_val_imputed\",\n",
    "#     F.when(F.col(\"max_price_val_imputed\").isNull(), F.lit(global_med_max)).otherwise(F.col(\"max_price_val_imputed\"))\n",
    "# )\n",
    "\n",
    "# # 6. Replace original columns and drop helper columns\n",
    "# df = df.drop(\"min_price_val\", \"max_price_val\") \\\n",
    "#        .withColumnRenamed(\"min_price_val_imputed\", \"min_price_val\") \\\n",
    "#        .withColumnRenamed(\"max_price_val_imputed\", \"max_price_val\") \\\n",
    "#        .drop(\"med_min_loc\", \"med_max_loc\")\n",
    "\n",
    "# # Log after imputation\n",
    "# null_min2 = df.filter(F.col(\"min_price_val\").isNull()).count()\n",
    "# null_max2 = df.filter(F.col(\"max_price_val\").isNull()).count()\n",
    "# print(f\"After price imputation: null min_price_val = {null_min2}, null max_price_val = {null_max2}, total rows = {df.count()}\")\n",
    "\n",
    "\n",
    "# # Log invalid price range parsing\n",
    "# invalid_price = df.filter(F.col(\"price_display_value\").isNotNull() &\n",
    "#                           F.col(\"min_price_val\").isNull() & F.col(\"max_price_val\").isNull())\n",
    "# print(f\"Invalid price ranges (remain in DF for now): {invalid_price.count()}\")\n",
    "# # You can df.select(\"price_display_value\").distinct().show(20,False) here to inspect formats.\n",
    "\n",
    "# # 6. Parse property_price\n",
    "# df = df.withColumn(\"property_price_val\", parse_amount_udf(F.col(\"property_price\")))\n",
    "\n",
    "# # 7. Parse property_area\n",
    "# df = df.withColumn(\"property_area_val\",\n",
    "#                    F.when(F.col(\"property_area\").isNotNull(),\n",
    "#                           F.regexp_extract(F.col(\"property_area\"), r\"([\\d\\.]+)\", 1).cast(DoubleType())\n",
    "#                          ).otherwise(None)\n",
    "# )\n",
    "\n",
    "# # 8. Parse size_range into min_size / max_size\n",
    "# df = df.withColumn(\"size_range_clean\", F.regexp_replace(F.col(\"size_range\"), \"sq\\\\.ft\\\\.?\", \"\")) \\\n",
    "#        .withColumn(\"size_parts\", F.split(F.col(\"size_range_clean\"), r\"\\s*-\\s*\")) \\\n",
    "#        .withColumn(\"min_size\", \n",
    "#                    F.when(F.size(F.col(\"size_parts\")) >= 1, \n",
    "#                           F.col(\"size_parts\").getItem(0).cast(DoubleType())).otherwise(None)) \\\n",
    "#        .withColumn(\"max_size\", \n",
    "#                    F.when(F.size(F.col(\"size_parts\")) >= 2, \n",
    "#                           F.col(\"size_parts\").getItem(1).cast(DoubleType())).otherwise(None)) \\\n",
    "#        .drop(\"size_range_clean\", \"size_parts\")\n",
    "\n",
    "# # 9. Cast latitude/longitude to Double\n",
    "# df = df.withColumn(\"latitude\", F.col(\"latitude\").cast(DoubleType())) \\\n",
    "#        .withColumn(\"longitude\", F.col(\"longitude\").cast(DoubleType()))\n",
    "\n",
    "# # 10. Cast bedrooms\n",
    "# df = df.withColumn(\"bedrooms\", F.col(\"bedrooms\").cast(IntegerType()))\n",
    "\n",
    "# # 11. Parse posted_date timestamp\n",
    "# df = df.withColumn(\n",
    "#     \"posted_timestamp\", \n",
    "#     F.to_timestamp(F.col(\"posted_date\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
    "# )\n",
    "\n",
    "# # 12. Parse possession_date: if present, parse “01 <month-year>”, else fallback to posted_timestamp date\n",
    "# df = df.withColumn(\n",
    "#     \"possession_date_parsed\",\n",
    "#     F.when(\n",
    "#         F.col(\"possession_date\").isNotNull(),\n",
    "#         F.to_date(F.concat(F.lit(\"01 \"), F.col(\"possession_date\")), \"dd MMM, yyyy\")\n",
    "#     ).otherwise(F.to_date(F.col(\"posted_timestamp\")))\n",
    "# )\n",
    "\n",
    "# # 13. Cast boolean-like columns\n",
    "# for bool_col in [\"is_active_property\", \"is_most_contacted\", \"seller_is_paid\"]:\n",
    "#     df = df.withColumn(bool_col,\n",
    "#                        F.when(F.col(bool_col).isin(True, False), F.col(bool_col).cast(BooleanType()))\n",
    "#                         .when(F.lower(F.col(bool_col)) == \"true\", F.lit(True))\n",
    "#                         .when(F.lower(F.col(bool_col)) == \"false\", F.lit(False))\n",
    "#                         .otherwise(None)\n",
    "#                       )\n",
    "\n",
    "# # 14. Trim/initcap for categorical columns\n",
    "# for cat_col in [\"region\", \"city\", \"state\", \"seller_type\", \"seller_name\", \"locality\"]:\n",
    "#     df = df.withColumn(cat_col, \n",
    "#                        F.when(F.col(cat_col).isNotNull(), F.initcap(F.col(cat_col))).otherwise(F.col(cat_col)))\n",
    "\n",
    "# # 15. Logical price consistency: only drop rows where both min/max exist and min > max\n",
    "# df = df.withColumn(\n",
    "#     \"price_consistent\",\n",
    "#     F.when(\n",
    "#         (F.col(\"min_price_val\").isNotNull() & F.col(\"max_price_val\").isNotNull()) &\n",
    "#         (F.col(\"min_price_val\") > F.col(\"max_price_val\")),\n",
    "#         False\n",
    "#     ).otherwise(True)\n",
    "# )\n",
    "# df = df.filter(F.col(\"price_consistent\")).drop(\"price_consistent\")\n",
    "# print(f\"After price consistency filter: {df.count()}\")\n",
    "\n",
    "# # 16. Check & drop null lat/long if truly critical\n",
    "# missing_geo = df.filter(F.col(\"latitude\").isNull() | F.col(\"longitude\").isNull()).count()\n",
    "# print(f\"Rows missing lat/long: {missing_geo}\")\n",
    "# # If geo is critical, drop; otherwise you may choose to impute or keep with nulls.\n",
    "# df = df.filter(F.col(\"latitude\").isNotNull() & F.col(\"longitude\").isNotNull())\n",
    "# print(f\"After dropping rows with null lat/long: {df.count()}\")\n",
    "\n",
    "# # 17. Fill bathrooms/bedrooms if needed\n",
    "# df = df.withColumn(\"bathrooms\", F.coalesce(F.col(\"bathrooms\"), F.lit(-1))) \\\n",
    "#        .withColumn(\"bedrooms\", F.coalesce(F.col(\"bedrooms\"), F.lit(-1)))\n",
    "\n",
    "# # 18. Drop intermediate raw fields\n",
    "# df_clean = df.drop(\"avg_price_per_sqft\", \"price_display_value\", \n",
    "#                    \"property_price\", \"property_area\", \"description\",\n",
    "#                    \"size_range\", \"posted_date\", \"possession_date\")\n",
    "\n",
    "# # 19. Rename parsed columns\n",
    "# df_clean = df_clean.withColumnRenamed(\"avg_price_per_sqft_val\", \"avg_price_per_sqft\") \\\n",
    "#                    .withColumnRenamed(\"property_area_val\", \"property_area_sqft\") \\\n",
    "#                    .withColumnRenamed(\"property_price_val\", \"property_price\") \\\n",
    "#                    .withColumnRenamed(\"posted_timestamp\", \"posted_date\") \\\n",
    "#                    .withColumnRenamed(\"possession_date_parsed\", \"possession_date\")\n",
    "\n",
    "# # 20. Final schema cast\n",
    "# target_schema = {\n",
    "#     \"id\": \"string\", \"address\": \"string\", \"long_address\": \"string\", \"locality\": \"string\", \n",
    "#     \"city\": \"string\", \"region\": \"string\", \"state\": \"string\", \"latitude\": \"double\", \n",
    "#     \"longitude\": \"double\", \"bedrooms\": \"int\", \"bathrooms\": \"int\", \"avg_price_per_sqft\": \"double\", \n",
    "#     \"min_price_val\": \"double\", \"max_price_val\": \"double\", \"property_price\": \"double\", \n",
    "#     \"property_area_sqft\": \"double\", \"min_size\": \"double\", \"max_size\": \"double\", \n",
    "#     \"posted_date\": \"timestamp\", \"possession_date\": \"date\", \"description_clean\": \"string\", \n",
    "#     \"brochure_url\": \"string\", \"from_url\": \"string\", \"url\": \"string\", \n",
    "#     \"is_active_property\": \"boolean\", \"is_most_contacted\": \"boolean\", \n",
    "#     \"seller_is_paid\": \"boolean\", \"seller_name\": \"string\", \"seller_phone\": \"string\", \n",
    "#     \"seller_type\": \"string\", \"subtitle\": \"string\"\n",
    "# }\n",
    "# for col_name, dtype in target_schema.items():\n",
    "#     if col_name in df_clean.columns:\n",
    "#         df_clean = df_clean.withColumn(col_name, F.col(col_name).cast(dtype))\n",
    "\n",
    "# print(f\"Final cleaned count before null-imputation: {df_clean.count()}\")\n",
    "# df_clean.printSchema()\n",
    "\n",
    "# # -------------------- Null Counts Logging --------------------\n",
    "# # Compute null counts for each column\n",
    "# null_counts = df_clean.select([\n",
    "#     F.count(F.when(F.col(c).isNull(), c)).alias(c) \n",
    "#     for c in df_clean.columns\n",
    "# ])\n",
    "# print(\"Null counts per column:\")\n",
    "# null_counts.show(truncate=False)\n",
    "\n",
    "# # -------------------- Logical Null Handling / Imputation --------------------\n",
    "# # Example strategies: adjust as per your needs.\n",
    "\n",
    "# # 1. String / categorical columns: fill null with \"Unknown\" (or \"\")\n",
    "# str_cols = [c for c, t in df_clean.dtypes if t == 'string']\n",
    "# for c in str_cols:\n",
    "#     # You may choose empty string \"\" instead of \"Unknown\"\n",
    "#     df_clean = df_clean.withColumn(c, F.when(F.col(c).isNull(), F.lit(\"Unknown\")).otherwise(F.col(c)))\n",
    "\n",
    "# # 2. Numeric columns: fill null with sentinel (-1) or median\n",
    "# # Identify numeric columns (double or int)\n",
    "# num_cols = [c for c, t in df_clean.dtypes if t in ('double','int') and c not in (\"latitude\",\"longitude\")]  # example: maybe keep lat/long nulls? but we dropped earlier\n",
    "# for c in num_cols:\n",
    "#     # Example: fill with median for that column. \n",
    "#     # Only if there are nulls; else skip.\n",
    "#     null_count = df_clean.filter(F.col(c).isNull()).count()\n",
    "#     if null_count > 0:\n",
    "#         # Compute median via approxQuantile\n",
    "#         try:\n",
    "#             median = df_clean.approxQuantile(c, [0.5], 0.01)[0]\n",
    "#         except:\n",
    "#             median = None\n",
    "#         if median is not None:\n",
    "#             df_clean = df_clean.withColumn(c, F.when(F.col(c).isNull(), F.lit(median)).otherwise(F.col(c)))\n",
    "#         else:\n",
    "#             # fallback sentinel\n",
    "#             df_clean = df_clean.withColumn(c, F.when(F.col(c).isNull(), F.lit(-1)).otherwise(F.col(c)))\n",
    "\n",
    "# # 3. Boolean columns: fill null with False (or leave null if you prefer)\n",
    "# bool_cols = [c for c, t in df_clean.dtypes if t == 'boolean']\n",
    "# for c in bool_cols:\n",
    "#     df_clean = df_clean.withColumn(c, F.coalesce(F.col(c), F.lit(False)))\n",
    "\n",
    "# # 4. Date/Timestamp columns: fill null with a default date or current date, or leave null.\n",
    "# # Example: fill posted_date nulls with current timestamp; possession_date nulls with posted_date\n",
    "# if \"posted_date\" in df_clean.columns:\n",
    "#     df_clean = df_clean.withColumn(\n",
    "#         \"posted_date\",\n",
    "#         F.when(F.col(\"posted_date\").isNull(), F.current_timestamp()).otherwise(F.col(\"posted_date\"))\n",
    "#     )\n",
    "# if \"possession_date\" in df_clean.columns:\n",
    "#     df_clean = df_clean.withColumn(\n",
    "#         \"possession_date\",\n",
    "#         F.when(F.col(\"possession_date\").isNull(), F.to_date(F.col(\"posted_date\"))).otherwise(F.col(\"possession_date\"))\n",
    "#     )\n",
    "\n",
    "# # After imputation, re-log null counts\n",
    "# null_counts_after = df_clean.select([\n",
    "#     F.count(F.when(F.col(c).isNull(), c)).alias(c) \n",
    "#     for c in df_clean.columns\n",
    "# ])\n",
    "# print(\"Null counts per column after imputation:\")\n",
    "# null_counts_after.show(truncate=False)\n",
    "\n",
    "# # -------------------- Final Output --------------------\n",
    "# df_properties_cleaned = df_clean\n",
    "# print(f\"Final cleaned count after imputation: {df_properties_cleaned.count()}\")\n",
    "# # df_properties_cleaned.show(5, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SILVER Real Estate",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}