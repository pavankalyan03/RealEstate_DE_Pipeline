
# ğŸ™ï¸ End-to-End Real Estate Data Engineering & Analytics Pipeline

A complete data engineering project that transforms raw real estate data from [Housing.com](https://housing.com) into actionable insights for Hyderabadâ€™s property market. The solution includes web scraping, cloud-based processing, Snowflake warehousing, and Power BI dashboards â€” delivering real-time business intelligence to buyers, investors, and analysts.

> âš ï¸ **Note:** This entire repository is designed to be executed within an **Apache Airflow environment**.
>  - ğŸ”¹ The **input data is not uploaded** here due to large file size. 
>  - ğŸ”¹ You must **create your own connections to Snowflake and Databricks** in the Airflow UI or via environment variables.
>  - ğŸ”¹ This project assumes that Airflow is set up using the **Astro CLI** or any standard Airflow installation.

>  ğŸ“š Learn and install Airflow with Astro CLI:
>  ğŸ‘‰ [Astro CLI Documentation](https://docs.astronomer.io/astro/cli/install-cli)



---

## ğŸ“Œ Problem Statement

The Hyderabad property market data is:
- Fragmented across platforms
- Inconsistent and messy
- Lacking locality-wise insights
- Difficult to analyze manually

### âœ… This project solves:
- Data fragmentation with unified pipelines  
- Manual analysis with automated ETL  
- Missing insights with real-time dashboards  
- Poor quality data with domain-specific cleaning  

---

## âš™ï¸ Architecture Overview

```text
Housing.com â†’ Apify â†’ Python â†’ AWS S3 â†’ Databricks (PySpark) â†’ Snowflake â†’ Power BI
```

### ğŸ’½ Medallion Architecture
- Bronze (Staging): Raw JSON in AWS S3
- Silver (Processing): Cleaned, structured data in Databricks
- Gold (Analytics): Star schema in Snowflake

